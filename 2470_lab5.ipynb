{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThiagoMueller/csci2470labs/blob/main/2470_lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1u5o3yfY09B"
      },
      "source": [
        "# **Debiasing Word Embeddings**\n",
        "\n",
        "The lab that we'll be doing today is based off a widely-cited paper called [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf), by Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai (2016).\n",
        "\n",
        "In the paper, they demonstrate two important things:\n",
        "1.   Word embeddings learn deep meanings and subtle interconnections between words, and that includes both overtly and covertly sexist meanings of words\n",
        "2.   There are ways to quantify and mitigate that bias\n",
        "\n",
        "We'll be roughly following along with the methodology of the paper — if parts of this lab are unclear or don't make sense, we strongly encourage you to read the paper and see if that clears things up.  Even if you aren't confused you should read the paper anyways (it's *super* interesting).\n",
        "\n",
        "**Note:** The interaction between language and gender-based discrimination is a complex topic that goes far beyond our ability to quantify and vectorize words. We also recognize that gender isn't a binary, and the understanding of gender exhibited in this lab is quite naive. Regardless, there have been [well-documented instances](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G) of programs exhibiting clearly sexist behaviors. The point of this lab is to name that dynamic and give you some tools to think about it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-Y2OdZnCK7S"
      },
      "source": [
        "## Part 0: Setup\n",
        "Run this next block of code to load the dataset/embedding and other relevant imports.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNXFzdR4IFzQ"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoh-qsiXMxM-"
      },
      "source": [
        "For this lab, we'll be using a truncated version of the [Global Vectors for Word Representation (GloVe) word embedding](https://nlp.stanford.edu/projects/glove/) dataset. These are 300-dimensional vectors that were trained on the Wikipedia corpus circa 2014 and an open source data repository of newswire text called Gigaword 5. There are 6 billion tokens (individual words) in the corpus.\n",
        "\n",
        "For the sake of ease, we've done some preprocessing of the data for you — instead of a 400,000-word embedding, you have a well-structured embedding of the 50,000 most common words. We've also removed all of the punctuation, and stored it compressed in the same folder as this lab so that it's accessible in Colab.\n",
        "\n",
        "When the notebook prompts you, go to the URL in the browser and log in with your Brown email account — you'll be given a randomly generated token to paste back into this lab in order to authenticate it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive_dir = '/content/drive'\n",
        "drive.mount(drive_dir, force_remount=True)"
      ],
      "metadata": {
        "id": "Yt4QdGJQqWeP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea29a6b0-b60b-47cd-ba7a-a70fc82d7150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL5S7zkqJmvh"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# load dataset to vector_dict (linked above)\n",
        "# main_dir = 'Shared drives/CS1470 TAs Fall 2022'\n",
        "# lab_dir = 'Labs/lab05_language_debiasing/released_lab' ## may change\n",
        "\n",
        "# TODO: fill in path stencil below, and download appropriate files to your\n",
        "# local colab dir\n",
        "local_colab_dir = 'MyDrive/YOUR_PATH_TO_COPIED_COLAB_HERE'\n",
        "\n",
        "path = f\"{drive_dir}/{local_colab_dir}/preprocessed_glove.pickle\"\n",
        "with open(path, 'rb') as f:\n",
        "    vector_dict = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bBHw8NMM_xh"
      },
      "source": [
        "Using `vector_dict` (which we just unpickled), we get a `vector_matrix` and a `word_list` with corresponding indices. These will be useful when we are trying to manually inspect some of the gender bias in GLoVe embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFKaiHWXM6GQ"
      },
      "source": [
        "word_list = []\n",
        "vector_matrix = np.zeros(shape=(50000, 300))\n",
        "for i, (word, vec) in enumerate(vector_dict.items()):\n",
        "  word_list.append(word)\n",
        "  vector_matrix[i] = vec\n",
        "\n",
        "print('vector_matrix of shape', vector_matrix.shape, ':\\n', vector_matrix)\n",
        "print('\\nword_list of length',  len(word_list),      ':\\n', word_list[:12])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpuYGlLwJsNv"
      },
      "source": [
        "## Part 1: The Initial Embedding\n",
        "\n",
        "We've implemented code below that will help you clearly observe how gender functions in this word embedding. English is not a language that embeds gender into most words. That means that some of the results reflect meaningful gendered linguistic differences (between the pronouns \"he\" and \"she\", for example). Other embeddings, however, clearly illustrate the sexist biases that are a part of language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdZAVJQeKkc-"
      },
      "source": [
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "def find_similar(vector_matrix, input_vector, num_examples=5):\n",
        "    \"\"\"\n",
        "    Use a basic similarity calculation (cosine similarity) to find\n",
        "    the closest words to an input vector.\n",
        "    \"\"\"\n",
        "    # compute cosine similarity of input_vector with everything else in our vocabulary\n",
        "    cosine_similarities = linear_kernel(input_vector, vector_matrix).flatten()\n",
        "    cosine_similarities /= np.linalg.norm(vector_matrix, axis=1)\n",
        "\n",
        "    # sort by cosine similarities, to get the most similar vectors on top\n",
        "    related_words_indices = [i for i in cosine_similarities.argsort()[::-1]]\n",
        "    return [index for index in related_words_indices][:num_examples]\n",
        "\n",
        "def get_gendered_analogies(word, num_examples=5, vector_dict=vector_dict):\n",
        "    \"\"\"\n",
        "    Use find_similar() to manually observe how gendered biases are encoded\n",
        "    into the embedding.\n",
        "    \"\"\"\n",
        "    assert word in word_list, f'\"{word}\" not in observed vocabulary'\n",
        "\n",
        "    print(f'He is to she as \"{word}\" is to: ')\n",
        "\n",
        "    # TODO: get a gender-shifted vector of the input word using the word2vec analogy method:\n",
        "    # she - he = x - word --> x = she - he + word\n",
        "    x =\n",
        "    x = np.reshape(x, (1, 300))  # since x is (300,) (1d), we must reshape to input vector size\n",
        "\n",
        "    ## Get vectors similar to that input x and print out the results\n",
        "    gender_shifted = find_similar(vector_matrix, x, num_examples)\n",
        "    for i in gender_shifted:\n",
        "        print(' -', word_list[i])\n",
        "\n",
        "get_gendered_analogies(\"programmer\")    # TODO: Try out some different words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[CHECK-OFF]**\n",
        "\n",
        "- **Expected:**\n",
        "```\n",
        "He is to she as \"programmer\" is to:\n",
        " - programmer\n",
        " - programmers\n",
        " - ...\n",
        " ```\n",
        "\n",
        "- Please discuss this with your TA!"
      ],
      "metadata": {
        "id": "1jVV9t8LhLxF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gINL6m-MBBgq"
      },
      "source": [
        "Change the word in the call to `get_gendered_analogies()` to see what gendered embeddings are encoded into GloVe. Note that this is the analogy \"he is to she as `input_word` is to `output_word`\"; if you want to try out some of your own, `input_word` should be one that is traditionally coded as \"male\". (Or, you can modify your implementation of `get_gendered_analogies()` above if you want to try a word traditionally coded as \"female\".)\n",
        "\n",
        "**The observed gender bias sometimes is an appropriate classifier in the situation. For example, try:**\n",
        "\n",
        "\n",
        "* brother\n",
        "* king\n",
        "* monastery\n",
        "* masculine\n",
        "\n",
        "\n",
        "**Other times the gender bias in the model reflects sexist societal stereotypes. For example, try:**\n",
        "* programmer\n",
        "* brilliant\n",
        "* handsome\n",
        "* doctor\n",
        "* superstar\n",
        "* snappy\n",
        "* pharmaceuticals\n",
        "* warrior\n",
        "* genius\n",
        "\n",
        "You should be seeing some interesting results if you've implemented `get_gendered_analogies` correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12DF-zb_LLxI"
      },
      "source": [
        "## Part 2: The Gender Subspace\n",
        "\n",
        "So from some qualitative observations of which words are associated with each other, it's pretty clear that GloVe has learned sexist connotations of words. But how should we think about gender quantitatively in this embedding?\n",
        "\n",
        "If we can come up with several vectors for words that are clearly gender-paired, we can identify a \"gender subspace\" of the 300-dimensional vector space that these words live in. In the paper, considered the following gender-paired, (relatively) neutral words:\n",
        "\n",
        "- she/he\n",
        "- her/his\n",
        "- woman/man\n",
        "- herself/himself\n",
        "- daughter/son\n",
        "- mother/father\n",
        "- sister/brother\n",
        "- gal/guy\n",
        "- girl/boy\n",
        "- female/male\n",
        "- feminine/masculine\n",
        "\n",
        "We can then use some observations from linear algebra to see how significant gender is in this embedding and what gender looks like quantitatively in the embedding space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWP8rUvMGitW"
      },
      "source": [
        "# generate a matrix full of random numbers to serve as a unassuming control subspace\n",
        "control_subspace = np.random.randn(11, 300)\n",
        "\n",
        "f_words = 'she her woman herself daughter mother sister  gal girl female feminine'.split()\n",
        "m_words = 'he  his man   himself son      father brother guy boy  male  masculine'.split()\n",
        "\n",
        "## Fill a matrix with the differences between the gender-paired words\n",
        "## — each entry roughly corresponds to the gender shift in this embedding\n",
        "## Hint: Consider using a list comprehension [f(x) for x in iterable] and zip()\n",
        "gender_subspace = np.array([\n",
        "    ## TODO\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6ZPbKQygRwK"
      },
      "source": [
        "Now we find and plot the covariance matrices and eigenvalues of both the control and the gender subspace matrix.\n",
        "\n",
        "The details and ramifications of the covariance matrix is beyond the scope of this class. If you're interested in reading up on it, consider [this guide](https://datascienceplus.com/understanding-the-covariance-matrix/). The short of it is as follows:\n",
        "\n",
        "The variance that you know and love is actually a specific case of covariance between the input matrix and itself. The more generalized ***covariance*** is the joint variability of two vectors, considering associated elements.\n",
        "\n",
        "Heuristically, if two vectors are highly covariant, they share a greater linear dependence (are more strongly related). In a random set of words, there likely is a random, but generally low, distribution of covariance values, as words may or may not be related to one another.\n",
        "\n",
        "All you need to know right now is that:\n",
        "\n",
        " - $Cov(X) = \\begin{bmatrix}\n",
        " Var(X_1) & Cov(X_1, X_2) & \\cdots \\\\\n",
        " Cov(X_2, X_1) & Var(X_2) & \\cdots  \\\\\n",
        " \\vdots & \\vdots & \\ddots  \\\\\n",
        " \\end{bmatrix}$\n",
        "\n",
        " - $Cov(X) \\sim X^TX$ for a zero-mean matrix $X$.\n",
        "\n",
        "Using this covariance matrix, we can compute an eigenvalue decomposition that helps to explain the following information:\n",
        " - What vector components are used to define our subspace.\n",
        " - How significant are those components in explaining the dataset.\n",
        "\n",
        "**Notes:**\n",
        "- Notice that for the covariance computation, you truncate along a specific dimension depending on the order you transpose your matrices:\n",
        "    - If your covariance matrix is `(embed_size, embed_size)`, it's the covariance associating the embedding entries. **[IMPORTANT] We want this one, because we'll want to use it to impact our word embeddings later!**\n",
        "    - If your covariance matrix is `(word_count, word_cound)`, it's the covariance associating the different words in your list.\n",
        "- For your computation, consider taking advantage of [`np.linalg.eig`](https://docs.scipy.org/doc/numpy/reference/routines.linalg.html).\n",
        "- The result of this process will lead to imaginary results. Please use `np.real` as appropriate to keep only the real components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu2oNdQRHrUY"
      },
      "source": [
        "def get_covariance_eigs(subspace):\n",
        "    '''\n",
        "    TODO: Get the eigenvalues for the input subspace\n",
        "    '''\n",
        "    space_covariance = None\n",
        "    e_vals, e_vecs = None, None\n",
        "    return space_covariance, (e_vals, e_vecs)\n",
        "\n",
        "bias_cov, (bias_evals, bias_evecs) = get_covariance_eigs(gender_subspace)\n",
        "ctrl_cov, (ctrl_evals, ctrl_evecs) = get_covariance_eigs(control_subspace)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Covariance Matrix Plot**"
      ],
      "metadata": {
        "id": "dJgQHU9kADNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(ncols=2, figsize=(16, 5))\n",
        "\n",
        "#Calculate the minimum and maximum\n",
        "vmin = min(ctrl_cov.min(), bias_cov.min())\n",
        "vmax = max(ctrl_cov.max(), bias_cov.max())\n",
        "\n",
        "plot = axs[0].imshow(ctrl_cov, vmin=vmin, vmax=vmax)\n",
        "axs[0].set_title('Control Covariance Plot')\n",
        "\n",
        "plot = axs[1].imshow(bias_cov, vmin=vmin, vmax=vmax)\n",
        "axs[1].set_title('Gender Covariance Plot')\n",
        "fig.colorbar(plot);"
      ],
      "metadata": {
        "id": "q7X9s7CDLJSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Eigenvalue Plot**"
      ],
      "metadata": {
        "id": "-Xea-6rmAMdF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex0Wx9miH9o7"
      },
      "source": [
        "fig, axs = plt.subplots(ncols=2, figsize=(16, 5))\n",
        "\n",
        "num_eigs = 10         ## Should yeild around 10 useful eigenpairs\n",
        "x = np.arange(num_eigs)\n",
        "\n",
        "axs[0].bar(x, sorted(ctrl_evals[:num_eigs], reverse=True))\n",
        "axs[0].set_title('Control Eigenvalues')\n",
        "\n",
        "axs[1].bar(x, sorted(bias_evals[:num_eigs], reverse=True))\n",
        "axs[1].set_title('Gender Subspace Eigenvalues')\n",
        "\n",
        "for ax in axs:\n",
        "    ax.set_xlabel('Eigenvalue Index')\n",
        "    ax.set_ylabel('Eigenvalue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[CHECK-OFF]:**\n",
        "\n",
        "- What does the plot of the control eigenvalues look like? The gender subspace eigenvalues? What are the differences between these plots and what do those differences illustrate?"
      ],
      "metadata": {
        "id": "2yA6yqnBzAAu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "764RiqrkjZ_0"
      },
      "source": [
        "## Part 3: Debiasing\n",
        "Now we're going to be debiasing the embeddings using properties of linear algebra. In this lab, we will doing a toy example — in the paper, they debias almost the entire dataset.\n",
        "\n",
        "From part 1, we have some words that return sexist analogies that we want to debias:\n",
        "* programmer\n",
        "* brilliant\n",
        "* handsome\n",
        "* doctor\n",
        "* superstar\n",
        "* snappy\n",
        "* pharmaceuticals\n",
        "* warrior\n",
        "* genius\n",
        "\n",
        "From part 2, we also have some sense of which vectors create the \"gender subspace\", as well as some word-pairs that define the subspace\n",
        "\n",
        "In the real world, the effect is more pervasive and subtle. It will likely be easier to explicitly identify the worlds that we *don't* want to debias so as to retain useful linguistic differences in gender (brother/sister, he/she, etc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL7zr8VvG7wU"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def test_bias(vector_dict, biased_words, equality_sets):\n",
        "    \"\"\"\n",
        "    A function that computes cosine similarities between the terms in the equality sets and the biased\n",
        "    words, illustrating how the terms we've listed above are coded as more masculine than feminine.\n",
        "\n",
        "    We will use this function again after we have debiased the biased words to illustrate what debiasing\n",
        "    actually means in this context.\n",
        "    \"\"\"\n",
        "    f_sims = []    ## Cumulative (global) statistics\n",
        "    m_sims = []\n",
        "\n",
        "    # for each biased word\n",
        "    for word in biased_words:\n",
        "        word_f_sims = []   ## Per-word statistics\n",
        "        word_m_sims = []\n",
        "\n",
        "        # calculate similarity to the masculine and feminine terms in the equality sets\n",
        "        for f_word, m_word in equality_sets:\n",
        "\n",
        "            w2vec = lambda w: np.array([vector_dict[w]]) ## maps to vector mtx\n",
        "\n",
        "            f_sim = cosine_similarity(w2vec(word), w2vec(f_word))\n",
        "            m_sim = cosine_similarity(w2vec(word), w2vec(m_word))\n",
        "\n",
        "            f_sims += [f_sim]\n",
        "            m_sims += [m_sim]\n",
        "\n",
        "            word_f_sims += [f_sim]\n",
        "            word_m_sims += [m_sim]\n",
        "\n",
        "        print(f'\"{word}\"')\n",
        "        print(\"   Similarity female-coded words:\", round(sum(word_f_sims)[0][0] / len(word_f_sims), 4))\n",
        "        print(\"   Similarity male-coded words:\", round(sum(word_m_sims)[0][0] / len(word_m_sims), 4))\n",
        "        print()\n",
        "\n",
        "    print(\"=================================================\")\n",
        "    print(\"Overall average similarity to female-coded words:\")\n",
        "    print(\"   \", round(sum(f_sims)[0][0] / len(f_sims), 4))\n",
        "    print(\"—————————————————————————————————————————————————————\")\n",
        "    print(\"Overall average similarity to male-coded words:\")\n",
        "    print(\"   \", round(sum(m_sims)[0][0] / len(m_sims), 4))\n",
        "\n",
        "# terms that have sexist encodings in GLoVe\n",
        "biased_words = \"programmer brilliant handsome doctor superstar brilliant\"\n",
        "# biased_words += \" snappy pharmaceuticals warrior genius\" ## More words, optional\n",
        "biased_words = biased_words.split()\n",
        "\n",
        "## Pairs of gendered terms without negative connotations.\n",
        "##    To neutralize the above, we will enforce that they are\n",
        "##    perpendicular to the space defined by these terms\n",
        "equality_sets = list(zip(f_words, m_words))\n",
        "\n",
        "# gives a quantitative understanding of the bias implicit in these embeddings\n",
        "test_bias(vector_dict, biased_words, equality_sets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NE00bc6jfkd"
      },
      "source": [
        "### **[CHECK-OFF]**\n",
        "- What do the variable cosine similarities numbers between the biased words and the gender-paired words show? Any first-impressions?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmWyfeuZQyb-"
      },
      "source": [
        "Up next, we're going to have to implement our projection routine.\n",
        "\n",
        "Here's a refresher about projection to new bases and normalization of vectors:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ysNZE0cNq3Ku80x5jIjVX1uNjmTYYd9f\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HAY8OA1Vn58"
      },
      "source": [
        "def project(v, basis):\n",
        "    out = np.zeros((300,))   ## We'll hard-code this in for clarity\n",
        "    for b in basis:\n",
        "        comp = np.dot(v, b)/(np.linalg.norm(b)**2) * b\n",
        "        out += np.real(comp)  ## Drop the imaginary component\n",
        "    return out\n",
        "\n",
        "def normalize(v):\n",
        "  return v / np.linalg.norm(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQGEfgjVmnTc"
      },
      "source": [
        "We neutralize the biased words by making them orthogonal to the gender subspace that we defined above. That means that they are perpendicular to explicitly gendered vectors, and also perpendicular to the gender subspace as a whole.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "To visualize what's happening here, let's imagine we have three vectors in 2d space:\n",
        "\n",
        "- The vectors for a gender-paired set of terms $v_{he}$ and $v_{she}$\n",
        "- The word we have observed as having bias (e.g. \"doctor\").\n",
        "\n",
        "Further, let's visualize the gender subspace that we've defined above as 1 vector.\n",
        "\n",
        "The graphic below illustrates that the cosine similarity between $v_{he}$ and $v_{doctor}$ is greater than the similarity between $v_{she}$ and $v_{doctor}$, indicating gender bias.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1zdlIoYpsxUwUV0Gz1CmYHoORCXtzB-7C\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9gfYdDfWmaR"
      },
      "source": [
        "What we are doing when we \"neutralize\" a biased term like \"doctor\" is:\n",
        "1. Find the projection of the vector into the gender subspace.\n",
        "    - This lets us see what parts of the embedding is gendered.\n",
        "2. Subtract the projection from the original vector.\n",
        "    - This removes the components that are aligned with the gender bias from the embedding.\n",
        "3. Normalize the resultant vector to conform like the others.\n",
        "\n",
        "After that, the neutralized vector should be roughly equidistant from both \"he\" and \"she\", and should have roughly the same cosine similarity.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1_EP0wofAr79bjbGm50htPP3LNGyCao3J\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrejP9nMDzQ_"
      },
      "source": [
        "import copy\n",
        "\n",
        "def neutralize(vector_dict, biased_words, basis):\n",
        "    '''\n",
        "    Neutralize the biased word entries based on the computer eigenvectors.\n",
        "    Update the resulting values in the output dictionary and return the result.\n",
        "    '''\n",
        "    vect_dict_w_norm = copy.deepcopy(vector_dict)\n",
        "\n",
        "    for word in biased_words:\n",
        "        ## TODO: Implement the routine\n",
        "        continue\n",
        "\n",
        "    return vect_dict_w_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0mjL8GRW0el"
      },
      "source": [
        "num_bases = 2   ## Figure out how many bases you'd like to neutralize w.r.t.\n",
        "basis = bias_evecs.T[:num_bases, :]\n",
        "vect_dict_w_norm = neutralize(vector_dict, word_list, basis)\n",
        "\n",
        "print('- Original Biased \"doctor\" vector: [', *np.round(vector_dict     ['doctor'][:5], 4), '... ]')\n",
        "print('- New Neutralized \"doctor\" vector: [', *np.round(vect_dict_w_norm['doctor'][:5], 4), '... ]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Check-Off]**\n",
        "```\n",
        "- Original Biased \"doctor\" vector: [ -0.2021 0.075 0.0237 -0.0078 -0.2637 ... ]\n",
        "- New Neutralized \"doctor\" vector: [ -0.0182 0.0157 0.0084 0.0023 -0.0501 ... ]\n",
        "```\n"
      ],
      "metadata": {
        "id": "Va7iReCaajCB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBbNfN9XmPPo"
      },
      "source": [
        "Now the cosine similarities between gendered terms for the previously sexist words are all closer to zero than before. This means that the words are orthogonal to the gender subspace and (in the eyes of the model) have less gendered connotations!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWBjmiJ4XGoh"
      },
      "source": [
        "test_bias(vect_dict_w_norm, biased_words, equality_sets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_gendered_analogies(\"doctor\", vector_dict=vect_dict_w_norm)"
      ],
      "metadata": {
        "id": "8F-PLRuehttn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_gendered_analogies(\"programmer\", vector_dict=vect_dict_w_norm)"
      ],
      "metadata": {
        "id": "mBKpjnON2DxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fTSBgqxcH2u"
      },
      "source": [
        "### **[CHECK-OFF]**\n",
        "\n",
        "- How have the cosine similarities between the biased words and the pairs of gendered words changed? What does that change signify?\n",
        "\n",
        "- Why is it important to only project onto a basis defined by a relatively small number of influential eigenvectors?\n",
        "\n",
        "- If we only projected a handful of words, `get_gendered_analogies` would predict something quite different despite the projection of doctor being the same. The results of this are shown below. Why might that be?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vect_dict_w_subset_norm = neutralize(vector_dict, biased_words, basis)\n",
        "get_gendered_analogies(\"doctor\", vector_dict=vect_dict_w_subset_norm)"
      ],
      "metadata": {
        "id": "m6bKwO5k3q2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTXZphVZ4e2i"
      },
      "source": [
        "## **Lipstick on a Pig**: Systemic Language Bias\n",
        "\n",
        "> #### \"Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them”\n",
        "\n",
        "While the debiasing methods you practiced reduce the cosine similarity between explicitly gendered terms and biased terms, it is important to note that because gender bias is a systemic phenomenon, there are deep limitations to this form of debiasing. A [2019 paper](https://arxiv.org/pdf/1903.03862.pdf) ***“Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them”*** directly references the paper we implemented and argues:\n",
        "\n",
        "> “Several recent works… propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between \"gender-neutralized\" words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.”\n",
        "\n",
        "We encourage you to read the full paper if you’re interested!\n",
        "\n",
        "Below are other excerpts that clarify the paper’s main points.\n",
        "\n",
        "**Why the authors believe reducing bias by its definition is an inadequate fix:**\n",
        "\n",
        "> “… Both methods and their results rely on the specific bias definition [the projection of a word $w$ on the ‘gender direction’: $w \\cdot (he - she)$]. We claim that the bias is much more profound and systematic, and that **simply reducing the projection of words on a gender direction is insufficient: it merely hides the bias**, which is still reflected in similarities between “gender-neutral” words (i.e., words such as “math” or “delicate” are in principle gender neutral, but in practice have strong stereotypical gender associations, which reflect on, and are reflected by, neighbouring words).”\n",
        "\n",
        "**How “the actual effect is most hiding the bias, not removing it”:**\n",
        "\n",
        "> “Our key observation is that, almost by definition, most word pairs maintain their previous similarity, despite their change in relation to the gender direction. The implication of this is that most words that had a specific bias before are still grouped together, and apart from changes with respect to specific gendered words, **the word embeddings’ spatial geometry stays largely the same**. In what follows, we provide a series of experiments that demonstrate the remaining bias in the debiased embeddings.”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[CHECK-OFF]**\n",
        "\n",
        "During the optimization routine, the model will try to optimize for the loss function in whatever way it thinks is best to minimize its error.\n",
        "\n",
        "- Can you think of any techniques to neutralize bias by modifying the training of a model?\n",
        "\n",
        "- Can you think of a way to ***force*** a model to have a bias of interest, or ***require*** it to incorporate specific information into its decision-making process? What might be the validity of doing that?\n",
        "\n",
        "- Can you think of areas where this lab and the Lipstick paper apply? In which applications is it important to reduce prediction biases? What are some broader social/regulatory measures that can be taken to check deep learning model bias?"
      ],
      "metadata": {
        "id": "GWs8YgjPVAoT"
      }
    }
  ]
}